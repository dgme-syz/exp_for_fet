{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\anaconda\\envs\\webui\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 100]\n",
    "# shuffle but not in place\n",
    "b = random.sample(a, len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 3, 100, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.functional import stat_scores\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# 假设 y_true 和 y_pred 是 PyTorch tensor 格式的标签\n",
    "y_true = torch.tensor([[1, 0, 1], [0, 1, 1], [1, 1, 0]])\n",
    "y_pred = torch.tensor([[1, 1, 0], [0, 1, 1], [1, 1, 0]])\n",
    "\n",
    "# 使用 TorchMetrics 计算多标签F1分数\n",
    "acc = Accuracy(num_classes=3, task=\"binary\", num_labels=3, average=\"none\")\n",
    "f1 = acc(y_pred, y_true)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\anaconda\\envs\\webui\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"E:/pretrained_models/datasets/figer/figer/1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 753/753 [00:02<00:00, 327.07ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 753/753 [00:02<00:00, 325.76ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [01:31<00:00, 45.94s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 652.61ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DGME/figer/commit/30d30988600b1aafe01208d48936680d1cce0b6a', commit_message='Upload dataset', commit_description='', oid='30d30988600b1aafe01208d48936680d1cce0b6a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"DGME/figer\", token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': [8, 13, 19], 'end': [11, 17, 23], 'labels': [['/train'], ['/organization', '/organization/company'], ['/rail/railway', '/organization', '/organization/company', '/rail', '/location']]}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"][3][\"mentions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [['It',\n",
       "   'includes',\n",
       "   'a',\n",
       "   'cover',\n",
       "   'of',\n",
       "   '\"',\n",
       "   'Low',\n",
       "   'Rider',\n",
       "   '\"',\n",
       "   'by',\n",
       "   'War',\n",
       "   'and',\n",
       "   'a',\n",
       "   'reworking',\n",
       "   'of',\n",
       "   'Muddy',\n",
       "   'Waters',\n",
       "   \"'\",\n",
       "   '\"',\n",
       "   'I',\n",
       "   'Just',\n",
       "   'Wanna',\n",
       "   'Make',\n",
       "   'Love',\n",
       "   'To',\n",
       "   'You',\n",
       "   '\"',\n",
       "   'as',\n",
       "   '\"',\n",
       "   'I',\n",
       "   'Just',\n",
       "   'Wanna',\n",
       "   'Get',\n",
       "   'High',\n",
       "   'with',\n",
       "   'You',\n",
       "   '\"',\n",
       "   '.'],\n",
       "  ['He',\n",
       "   'was',\n",
       "   'hospitalized',\n",
       "   'for',\n",
       "   'four',\n",
       "   'days',\n",
       "   'at',\n",
       "   'the',\n",
       "   'Mayo',\n",
       "   'Clinic',\n",
       "   'in',\n",
       "   '1964',\n",
       "   ',',\n",
       "   'and',\n",
       "   'for',\n",
       "   'three',\n",
       "   'weeks',\n",
       "   'in',\n",
       "   '1966',\n",
       "   '.']],\n",
       " 'senid': [0, 1],\n",
       " 'mentions': [{'start': [15],\n",
       "   'end': [17],\n",
       "   'labels': [['/person/musician',\n",
       "     '/person/actor',\n",
       "     '/person/artist',\n",
       "     '/person']]},\n",
       "  {'start': [8],\n",
       "   'end': [10],\n",
       "   'labels': [['/organization',\n",
       "     '/organization/company',\n",
       "     '/building/hospital',\n",
       "     '/building',\n",
       "     '/location']]}],\n",
       " 'fileid': ['train.jsonl', 'train.jsonl']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"E:/pretrained_models/Qwen/Qwen2___5-0___5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nI want to book a restaurant for 2 people.<|im_end|>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"I want to book a restaurant for 2 people.\"}],\n",
    "    tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import register_template, AutoCLS\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process(\n",
    "    examples: dict, \n",
    "    tokenizer: AutoTokenizer,\n",
    "    order: int, \n",
    ") -> dict:\n",
    "    mentions, tokens = examples[\"mentions\"], examples[\"tokens\"]\n",
    "    text, label, num_samples = [], [], len(mentions)\n",
    "    \n",
    "    ord = [\"xx\", \"y\"]\n",
    "    \n",
    "    def access_single(i: int):\n",
    "        t, l = [], []\n",
    "        mention, token = mentions[i], tokens[i] # there may be multiple mentions\n",
    "        num_entity = len(mention[\"start\"])\n",
    "        for j in range(num_entity):\n",
    "            start, end, labels = mention[\"start\"][j], mention[\"end\"][j], mention[\"labels\"][j]\n",
    "            sentence = token\n",
    "            entity = sentence[start:end]\n",
    "            t.append(register_template(\n",
    "                sentence=\" \".join(sentence), mention=\" \".join(entity), cls_ord=ord\n",
    "            ))\n",
    "            l.append(labels)\n",
    "        return t, l\n",
    "   \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        res = executor.map(access_single, range(num_samples))\n",
    "        for t, l in res: text.extend(t), label.extend(l)\n",
    "        \n",
    "    return {\"text\": text, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =process(\n",
    "    ds[\"train\"][:2], \n",
    "    tokenizer, \n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task]: Fine-grained entity classification\n",
      "[sentence]: It includes a cover of \" Low Rider \" by War and a reworking of Muddy Waters ' \" I Just Wanna Make Love To You \" as \" I Just Wanna Get High with You \" .\n",
      "[entity]: Muddy Waters\n",
      "[entity types]: xx y\n",
      "[Classification Result]: \n",
      "[Warning]: Just output nothing except entity types above, separate them by one space, there may be more than one answer\n"
     ]
    }
   ],
   "source": [
    "print(x[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/organization', '/organization/company', '/building/hospital', '/building', '/location']\n"
     ]
    }
   ],
   "source": [
    "print(x[\"label\"][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'senid', 'mentions', 'fileid'],\n",
       "    num_rows: 1505765\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "a.extend([4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", \"E:/pretrained_models/Qwen/Qwen2___5-0___5B-Instruct\", \n",
    "    device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'xx war'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a excellent linguist, you can finish the following task well! Also, you need to recognize some entity types are relative.\"},\n",
    "    {\"role\": \"user\", \"content\": x[\"text\"][0]},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=256)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.rfind('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"E:/nlp/toy/temp_exp/sft_data/figertest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '[Task]: Fine-grained entity classification\\n[sentence]: A handful of professors in the UW Department of Chemistry are being recognized by the American Association for the Advancement of Science ( AAAS ) for their efforts and contributions to the scientific community .\\n[entity]: UW\\n[entity types]: / person musician artist director author athlete actor architect doctor politician soldier engineer monarch coach religious_leader terrorist location city country cemetery province body_of_water county bridge language organization company sports_league sports_team educational_institution airline terrorist_organization fraternity_sorority people ethnicity written_work software product computer weapon airplane ship spacecraft car instrument mobile_phone engine_device camera government government political_party park government_agency art film broadcast_program game geography island mountain glacier music military train rail railway building hospital airport sports_facility restaurant hotel theater power_station library dam event military_conflict attack natural_disaster terrorist_attack sports_event election protest title award law astral_body internet website disease chemistry news_agency time transportation road education educational_degree department transit broadcast_network broadcast tv_channel religion religion finance currency stock_exchange food livingthing animal living_thing god metropolitan_transit transit_line play body_part medicine medical_treatment drug symptom newspaper computer algorithm programming_language visual_art color biology\\n[Classification Result]: \\n[Warning]: Just output nothing except entity types above, separate them by one space, there may be more than one answer',\n",
       " 'output': ['organization', 'educational_institution']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
